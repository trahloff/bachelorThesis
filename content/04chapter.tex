\chapter{Research Method}


%===================================================================================================%
\section{Introduction}
%===================================================================================================%

After exploring the academic body of knowledge in chapter \ref{chp:background}, 


%===================================================================================================%
\section{Method Evaluation}
%=======================================================%

There are several competing methodological approaches to assess the aptness of technologies in the domain of information systems. In the following section four of these are introduced, discussed and evaluated for further use:

    
    %-------------------------------------%
    \subsection{Task-Technology-Fit}
    %-------------------------------------%
    \begin{figure}[ht]
        \includegraphics[width=0.7\linewidth]{images/methodology/ttf.jpg}\centering
        \caption
        [Task-Technology-Fit Model]
        {Task-Technology-Fit Model \cite{Goodhue1995Task-TechnologyPerformance}}
    \end{figure}
    
    One of the oldest holistic models for assessing the fit of a given technology is called the "Task-Technology-Fit" (TTF) model. It originates in 1995 and is based on theories of user attitudes, beliefs and behaviors. It implies that a better fit of a technology for a task will result in increased utilization and will have a positive performance impact. Goodhue and Thompson developed a measure that consists of the eight factors:
    \begin{enumerate}
        \item[\textbf{1}] Quality
        \item[\textbf{2}] Locatability
        \item[\textbf{3}] Authorization
        \item[\textbf{4}] Compatibility
        \item[\textbf{5}] Ease of Use/Training
        \item[\textbf{6}] Production Timeliness
        \item[\textbf{7}] Systems Reliability
        \item[\textbf{8}] Relationship with Users
    \end{enumerate}
    It is apparent that these eight factors are focused on the user of the technology and rely on subjective user-perception. Although Goodhue and Thompson developed their methodology for the individual level of analysis, Zigurs and Buckland refined it for application on group level.\autocite{Zigurs1998AEffectiveness} As mentioned before, this basic TTF model heavily relies on the subjective perception of the user and therefore often operates with surveys or interviews. Since TTF is a very generalistic and holostic approach, it was applied on various domains of information systems and has been combined or used with other related models with great success.\\ 
    However, it is very uncommon to use the TTF in its original, unaltered form but rather modify it to suit the purpose of the particular study since different fields of study require different validation methods. For the technical evaluation of serverless architectures in the context of IoT Event-Stream-Processing the TTF perspective of defining \textit{Task Characteristics} and \textit{Technology Characteristics} to assess the fit and therefore the \textit{Performance Impacts} and \textit{Utilization} but the suggested validation methods (i.e., survey or interview) are not applicable for this study. It is therefore necessary to look at different models and define a research methodology that is fitting for the study at hand. 
    
     %-------------------------------------%
    \subsection{Fit-Viability Model}
    %-------------------------------------%
    \begin{figure}[ht]
        \includegraphics[width=0.7\linewidth]{images/methodology/fvm.jpg}\centering
        \caption
        [Fit-Viability-Model]
        {Fit-Viability-Model \cite{Liang2007AdoptionModel}}
    \end{figure}
    The "Fit-Viability Model" (FVM) for evaluating organizational adoption of technologies was originally proposed by Tjan in 2001 and refined by Liang in 2007. As an extension of the Task-Technology-Fit model it adopts the technology's fit but also introduces the "viability" dimension. Similarly to the approach outline in the last paragraph, FVM assesses the fit be evaluating how well the studied technology fits for the given task in order to gain insights on how well it impacts the performance and utilization. The added dimension of viability is very interesting and important for this study since it includes the economic perspective on the solution and also enables the researcher to focus on the realistic implementation and use of the technology. Similarly to TTF, it is rather vague in recommending specific validation methods and needs to be modified to fit the study. 
    
    %-------------------------------------%
    \subsection{Prototyping}
    %-------------------------------------%
    "Prototyping" refers to the practice of building functioning versions of a system in order to observe its behaviour and derive conclusions on how the productive version would behave.\autocite{Budde1992Prototyping} Since it is a common approach for business and hobbyists alike, a concise and generally accepted definition is hard to find because the understanding can differ vastly depending on the person or organization using it. For research purpose, this paper focuses on the two most important academic sources on the topic: Budde (1992) and Nielsen (1993).\\
    Nielsen defines two different types of prototypes:
    
    \begin{figure}[ht]
        \includegraphics[width=0.7\linewidth]{images/methodology/nielsenProto.PNG}\centering
        \caption
        [The two dimensions of prototyping]
        {The two dimensions of prototyping \cite{Nielsen1993UsabilityEngineering}}
    \end{figure}
    
    \begin{enumerate}
        \item \textbf{Horizontal Prototype}\\
            By reducing the level of functionality, the result of horizontal prototyping is \blockquote{a simulation of the interface where no real work can be performed. [...] This would mean that users should be able to execute all navigation and search commands but without retrieving any real information as a result of these commands}.\autocite{Nielsen1993UsabilityEngineering} The goal is to test the entire user interface without needing to develop the underlying systems. The main advantage of horizontal prototyping is that it provides a rapid feedback-loop between researcher and user by reducing the development time but still delivering a fully functional prototype. In addition, a quick, cost-effective confirmation of the requirements and system scope can be done and first estimates of development time, cost and effort conducted. Moreover, it is an excellent way to showcase the running system to gain a business advantage.\autocite{Nielsen1993UsabilityEngineering}
        \item \textbf{Vertical Prototype}\\
            A vertical prototype is a system that features in-depth functionality but only for a few selected features. Hence, it only provides sufficient test data for a limited part of the system, but it will be tested thoroughly under near real-life conditions. For instance, a vertical prototype of a concert ticket system could feature a fully functional API and processing unit but mocks the database in order to assess only the API and processing unit. 
    \end{enumerate}
    
    Prototyping can be the used to solve various problems and depending on the goal the overall approach can differ vastly. Floyd\autocite{Floyd1984APrototyping} distinguishes between three particular styles of prototyping relating to different goals:
    
    \begin{enumerate}
        \item \textbf{Exploratory Prototyping}\\
            When the problem is unclear, exploratory prototyping is an apt approach for testing initial ideas in order to gain perspective on user and management requirements. Usually, the main focus is to examine a broad variety of design options and to explore these options without premature assumptions or restrictions. As a result, the developer can "play" with various design choices and gain insight into the users' perception of the application. 
        \item \textbf{Experimental Prototyping}\\
            This form of prototyping is closely related to \textit{exploratory prototyping} but focuses on the technical implementation of the development goal. By designing reference systems within the defined problem's scope, the researcher gains insight into suitability and feasibility of a particular application system. The most common approach is to define an experimental environment with disclosed parameters, design several prototypes, determine indices to measure and lastly observe the prototypes' behavior in the experimental environment. Afterwards, the results from each prototype are compared and a technical recommendation is formulated. 
        \item \textbf{Evolutionary Prototyping}\\
            Contrary to the two types mentioned before, with evolutionary prototyping, the prototype is not merely used as a single-use tool in the context of an experiment or research endeavour, but is rather part of a continuous process for adapting a system to various constraints and environment properties. As a result, software development is not seen as a self-contained venture, instead it is organized as a continuous improvement cycle where the prototype is refined in an iterative process until the prototype becomes the final productive release. The aim is to improve the prototype throughout the steps of development in order to eliminate the differences between prototypes and application systems. In comparison to exploratory and experimental prototyping, evolutionary prototyping is therefore very focussed on working towards a productive release. The former methods are hence called "throw-away prototyping" approaches.\autocite{Crinnion1991EvolutionaryMethodology}
    \end{enumerate}
    
    %-------------------------------------%
    \subsection{Requirements Engineering}\label{sssec:reqEng}
    %-------------------------------------%
    "Requirements engineering is the branch of software
    engineering concerned with the real-world goals for,
    functions of, and constraints on software systems. It
    is also concerned with the relationship of these
    factors to precise specifications of software behavior,
    and to their evolution over time and across software
    families."\autocite{Zave1997ClassificationEngineering}
    
    This definition is very appealing for a number of reasons. To begin with, it emphasizes the focus on "real-world goals" that have to addressed in order to create value. These goals define \textit{why} the system should be built and \textit{what} the system's purpose is. In addition, it mentions "precise specification" which provide the the basis for requirements that shape how the software will behave. \textit{Requirements Engineering} is therefore the method of defining and documenting requirements for a software. There is no specific, generally accepted\autocite{Budde1992Prototyping}$^{,}$\autocite{Sommerville1999RequirementsGuide}$^{,}$\autocite{Sommerville2010SoftwareEngineering} toolset of best practices and methods but rather a common understanding what requirements engineering should achieve: 
    \begin{itemize}
        \item Identification of Requirements\\
            Understand the features and functionality the task's environment demands
        \item System Modeling (see \textit{Prototyping})\\
            Design a reference system 
        \item Requirements Specification\\
            Specify the identified requirements further
        \item Requirements Validation\\
            Validate the identified and specified requirements
    \end{itemize}
    
    To validate the identified requirements, a quantitive expert survey will be conducted. A "Likert Scale" \autocite{Likert1932AAttitudes} will be utilized as a proven method to create a standardized questionnaire that presents the respondents all requirements and allows them to rate each requirement. The goal is to objectively evaluate which system properties will be tested in the following sections.
    
    The Likert scale is one if the most frequently used form of attitude measurement in survey research. Like most psychometric scale measures, the Likerts scale employs the approach to list multiple statements that the interviewees can rank corresponding to their level of agreement. An example can be seen at Figure \ref{fig:likertExample}. Each item uses the same bipolar set of response option expressing agreement or disagreement with the statement (e.g., \textit{Good performance is important for a proper database}. Both ends of the scale are labeled accordingly. \autocite{Brill2008LikertScale}
    
    \begin{figure}[ht]
        \includegraphics[width=\linewidth]{images/methodology/likert.PNG}\centering
        \caption
        [Likert Scale Example]
        {Likert Scale Example}
        \label{fig:likertExample}
    \end{figure}
    
    

\begin{minipage}{\textwidth}
    %-------------------------------------%
    \subsection{Conclusion}
    %-------------------------------------%
    
    \subsubsection{Approach}
    Evaluation of two ESP Data-Consumer Approaches
    \begin{enumerate}
        \item \textbf{Requirements Engieering}\\
            Based on Suitability (TTF) \& Viability (FVM)
            \begin{enumerate}
                \item[I] \textbf{Requirements Identification} (LiteratureReview)\\
                    $\longrightarrow$ \textbf{Requirements Specification}(Hypothesis)
                \item[II] \textbf{Requirements Validation/Selection}(Quantitative Survey, Likert Scale)
            \end{enumerate}
        \item \textbf{Environment/Task Modelling}
        \item \textbf{Prototype Modelling}
        \item \textbf{Evaluation of Prototypes} (Based on Requirements)
        \item \textbf{Recommendation}
    \end{enumerate}
    
    After evaluating different methods and their characteristics, an approach combined out of components from the Task-Technology-Fit Model and Fit-Viability Model is chosen to define environment attributes and conditions. Using literature research combined with requirement engineering, indices and variables to be measured will be determined. Requirements that are identified via this approach will be validated through a quantitive survey. The suitability and viability of the examined prototypes will be decisive for the architectural recommendation and will result in an answer to the research question and objective.\\
    Since this study aims to answer the question whether and under which circumstances serverless architectures are a good fit for IoT Event-Stream-Processing, an experimental, vertical prototype is best suited as a method of validation. 
    
\end{minipage}

%***************%
\section{Requirement Engineering and Operationalization}\label{sec:operationalization}
%***************%

\begin{figure}[ht]
    \includegraphics[width=\linewidth]{images/methodology/viabFit.PNG}\centering
    \caption
    [Viability-Fit Matrix]
    {Viability-Fit Matrix \cite{Liang2004IntroductionApplications}}
\end{figure}

In the following section, concepts and indices for measuring suitability and viability will be introduced. The insights presented in chapter \ref{chp:background} will be the foundation for the requirements and operators since the current academic and professional understanding of the topic will be assumed as correct and holistic. The steps from section \ref{sssec:reqEng} will be shortened for the upcoming sections on suitability and viability, since \textit{System Modeling} will be done in chapter \ref{chp:prototyping} and validation is discussed in chapter \ref{chp:suitabilityAssessment} and \ref{chp:viabilityAssessment} respectively.  

%***************%
\subsection{Suitability Assessment}
%***************%

The author defines \textit{suitability} as the measure of how well the prototype fits the environments demands and tasks characteristics. Quantifiably measurable aspects will be compared directly while disclosing the desirable value. Qualitatively measurable aspects will be discussed and deductively compared in order to study the prototypes' differences. 

\begin{figure}[ht]
    \includegraphics[width=0.7\linewidth]{images/methodology/ttf.jpg}\centering
    \caption
    [Task-Technology-Fit Model]
    {Task-Technology-Fit Model \cite{Goodhue1995Task-TechnologyPerformance}}
\end{figure}

\subsubsection{Identification of Requirements / Task Characteristics}

As discussed in chapter \ref{chp:background}, IoT Event-Stream-Processing entails a distinctive set of task and environment characteristics. It is characterized by an enormously high amount of client nodes that send data in a timely ordered fashion. These messages have to be processed in near real-time to avoid a system wide congestion that would result in a degradation of service and possible loss of messages. The API Gateway's performance is mostly neglectable in a real-world scenario since the computational heavy-lifting is done by the data-consumer, this characteristic results in a requirement for a processing architecture that can seamlessly handle the computational load that is resulting from any data ingress.\\
Since it is often not possible to estimate the exact amount of messages and therefore the exact amount of required compute power, the data-consumer's capacity has to \textit{automatically scale} with the load. The data ingress level can vary in short periods of, hence a manual scaling is not desirable.

Consequently, the architecture must be \textit{inherently event-driven}. If not, a fully automated scaling strategy is not possible by definition. Moreover, an event-driven approach results in an architecture that is better equipped to respond to elastic workloads, such as the ones found in IoT environments. As previously discussed, it is necessary to immediately process incoming information in an IoT context to derive valuable insights. An event-driven system is therefore well suited for this task because it \textit{minimizes the latency} between the moment of receiving the message and processing it because the information flow is strictly left-to-right. Other alternatives would introduce a lag-time by requiring the processing part of the architecture to query the data storage for new data. 

Equally important is the system's ability to "self-heal", meaning the capability to mitigate components collapse and to to maintain an operational state. This resulting \textit{failure resilience} of data-consumers is vital for an efficient Event-Stream-Processing architecture because a lack thereof will result in massive backpressure on the downstream services and API Gateway and leads to an inevitable system-wide congestion.\\
Tightly related is the tasks demand for a \textit{high throughput}. It can directly derived from the overarching need to avoid a message congestion at any cost. Naturally, a higher throughput is desirable and will lead to an increased performance and overall better suited system for the task.

Lastly, it is a characteristic of IoT Event-Processing-Tasks in a real-world business context, that cost are often calculated in a very granular fashion. Since the IoT system's monetary value is often contingent on the amount of messages processed, it is, from a business perspective, very desirable to have a system which generates costs that are directly correlated to the amount of messages processed, too. Hence, a \textit{dynamic billing based on computation} is a requirement that is not pivotal, but should be considered nonetheless.

\subsubsection{Specification of Requirements}

The identified requirements are therefore as follows:

\begin{enumerate}
    \item \textbf{Automated/Limitless Scaling}\\
        (Qualitatively measured)
    \item \textbf{Event-Driven Design}\\
        (Qualitatively measured) 
    \item \textbf{Low Latency}\\
        (Quantifiably measured) Message Received - Message Processed
    \item \textbf{Failure Resilience}\\
        (Qualitatively measured) Automatic Failover Strategies, Outage Mitigation, Self-Healing Capabilities
    \item \textbf{Message Throughput}\\
        (Quantifiably measured) 
\end{enumerate}



%***************%
\subsection{Viability Assessment}
%***************%

\textit{Viability} measures the extent of value-added by the technology in comparison to the cost of operation, development, implementation and so forth. Quantifiably measurable aspects will be compared directly while disclosing the desirable value. Qualitatively measurable aspects will be discussed and deductively compared in order to study the prototypes' differences. 

viability  measures the extent to which the environment or organization is ready fo r the application



\begin{figure}[ht]
    \includegraphics[width=0.7\linewidth]{images/methodology/fvm.jpg}\centering
    \caption
    [Fit-Viability-Model]
    {Fit-Viability-Model \cite{Liang2007AdoptionModel}}
\end{figure}


\subsubsection{Identification of Requirements / Task Characteristics}

As discussed in chapter \ref{chp:background},

\subsubsection{Specification of Requirements}

\begin{enumerate}
    \item \textbf{Ease of Development}\\
        (Qualitatively measured) Including: Quality/Comprehensiveness of Documentation, Available Tooling, Ease of Testing (Unit, Integration, User Acceptance, etc.), Debugging, ...
    \item \textbf{Ease of Operation}\\
        (Qualitatively measured) Including: Monitoring, Deployment, Versioning, Performance Optimization, ...
    \item \textbf{Feature Velocity}\\
        (Qualitatively measured) 
    \item \textbf{Low Vendor Lock-In}\\
        (Qualitatively measured) 
    \item \textbf{Cost}\\
        (Quantifiably measured) Including: Overall Cost of Operation/Ownership, Cost-Control, Ease of Billing/Payment, Total Costs of Maintenance, ...
\end{enumerate}


\subsection{Requirements Validation}

In order to select a set of important system's properties for the prototype evaluation, a quantitative survey was conducted employing a Likert scale. Ten Likert items were derived from the suitability and viability requirement identification in the previous steps. The Likert items are as follows:

\begin{enumerate}[nolistsep]
    \item \textbf{Ease of Development}\\
        Including: Quality/Comprehensiveness of Documentation, Available Tooling, Ease of Testing (Unit, Integration, User Acceptance, etc.), Debugging, ...
    \item \textbf{Ease of Operation}\\
        Including: Monitoring, Deployment, Versioning, Performance Optimization, ...
    \item \textbf{Feature Velocity}
    \item \textbf{Low Vendor Lock-In}
    \item \textbf{Cost}\\
        Including: Overall Cost of Operation/Ownership, Cost-Control, Ease of Billing/Payment, Total Costs of Maintenance, ...
    \item \textbf{Automated/Limitless Scaling}
    \item \textbf{Event-Driven Design}
    \item \textbf{Low Latency}\\
         Message Received - Message Processed
    \item \textbf{Failure Resilience}\\
         Automatic Failover Strategies, Outage Mitigation, Self-Healing Capabilities
    \item \textbf{Message Throughput}
\end{enumerate}

As bipolar response categories the set \textit{Not Important} and \textit{Very Import} were chosen. In order to prevent respondents from so called "fence-sitting" (choosing the most neutral response option), an even number of scale points was used. To improve the reliability furthermore, the forced-choice response set was extended to six feedback options since a low scale score variability is expected.\autocite{Brill2008LikertScale} Similarly, a set of consecutive integers (i.e., 1, 2, 3, 4, 5, 6) was used to label the response scale to emphasize the equidistance between the options. Consequently, this setup results in an ordinal level of measurement, which can be processed by parametric statistics to gain insights on how to construct the test cases. \\
The evaluation of this survey's results will lead to one distinct system characteristic each for \textit{suitability} and \textit{viability}. For each domain, the system property with the highest measured importance among the respondents will be chosen due to the limited time and scope of this research. \\
Likert scales are subject to several academic inaccuracies and response distortion. To begin with, central tendency (respondents tend to choose the most neutral and therefore most central response option\autocite{Schumacker2013CentralDispersion}) can occur due to the interviewees impulse to avoid extreme response categories. In addition, acquiescence bias can distort the results. It refers to the tendency of respondents to choose to agree with statements because of the intention to choose what they think is the appropriate or objectively "correct" answer.\autocite{Costello2015AcquiescenceEducation}\highcomma\autocite{Aichholzer2015ControllingTests} Similarly, social desirability bias may occur when respondents try to portray themselves in a specific way.\autocite{Grimm2010SocialBias} \\
Since this survey does not address personal, moral or ethical topics, the biases mentioned above will not have a drastic influence on the results. Their impact is therefore neglectable. 




%===================================================================================================%
\section{Summary}
%===================================================================================================%

\begin{enumerate}[nolistsep]
        \item \textbf{Requirements Engieering}\\
            Based on Suitability (TTF) \& Viability (FVM)
            \begin{enumerate}
                \item[I] \textbf{Requirements Identification} (LiteratureReview)\\
                    $\longrightarrow$ \textbf{Requirements Specification}(Hypothesis)
                \item[II] \textbf{Requirements Validation/Selection}(Quantitative Survey, Likert Scale)
            \end{enumerate}
        \item \textbf{Environment/Task Modelling}
        \item \textbf{Prototype Modelling}
        \item \textbf{Evaluation of Prototypes} (Based on Requirements)
        \item \textbf{Recommendation}
\end{enumerate}

After evaluating different methods and their characteristics, an approach combined out of components from the Task-Technology-Fit Model and Fit-Viability Model is chosen to define environment attributes and conditions. Using literature research combined with requirement engineering, indices and variables to be measured will be determined. Requirements that are identified via this approach will be validated through a quantitative survey. 

As a methodological approach for the quantitive survey, a Likert scale was employed since the study benefits from its academic advantages and the impact of its response distortions if minimal. The bipolar response option set \textit{Not Important} and \textit{Very Important} was used to let the respondents (technical experts in the field of IoT, Event-Stream-Processing and Serverless) express their personal prioritization of various system properties. To emphasize the equidistance between the options, a consecutive set of integers (ranging from 1 to 6) was employed as scale labels. The Likert items are as follows:

\begin{enumerate}[nolistsep]
    \item \textbf{Ease of Development}\\
        Including: Quality/Comprehensiveness of Documentation, Available Tooling, Ease of Testing (Unit, Integration, User Acceptance, etc.), Debugging, ...
    \item \textbf{Ease of Operation}\\
        Including: Monitoring, Deployment, Versioning, Performance Optimization, ...
    \item \textbf{Feature Velocity}
    \item \textbf{Low Vendor Lock-In}
    \item \textbf{Cost}\\
        Including: Overall Cost of Operation/Ownership, Cost-Control, Ease of Billing/Payment, Total Costs of Maintenance, ...
    \item \textbf{Automated/Limitless Scaling}
    \item \textbf{Event-Driven Design}
    \item \textbf{Low Latency}\\
         Message Received - Message Processed
    \item \textbf{Failure Resilience}\\
         Automatic Failover Strategies, Outage Mitigation, Self-Healing Capabilities
    \item \textbf{Message Throughput}
\end{enumerate}

After calculating the average response value for each item, the two items with the highest value in the categories \textit{suitability} and \textit{viability} will be chosen to be examined in the next steps.\\
A testing environment and task will be constructed based on the two chosen system properties from the previous step. 
The suitability and viability of the examined prototypes will be decisive for the architectural recommendation and will result in an answer to the research question and objective.
Since this study aims to answer the question whether and under which circumstances serverless architectures are a good fit for IoT Event-Stream-Processing, an experimental, vertical prototype is best suited as a method of validation. 